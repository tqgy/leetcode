package com.careerup.carrot;

import java.util.*;

/**
 * You are given a 2D array of strings representing a retail sales table.
 * 
 * prodId | sales | cost | state | timestamp 
 * A1 | 10 | 5 | CA | 2024-01-01 
 * A2 | 20 | 8 | NY | 2024-01-01 
 * A1 | 15 | 7 | CA | 2024-01-02
 * 
 * Input: salesTable ‚Üí String[][] Row 0 = header Rows 1..n = data All values are
 * strings
 * 
 * Columns always include: prodId sales cost state (or country) timestamp
 * (YYYY‚ÄëMM‚ÄëDD)
 * 
 * pivotColumn ‚Üí String Examples: "sales" "cost" "state" "timestamp"
 * 
 * Tasks 1. 
 * Compute total sum of a numeric column 
 * Example: sum("sales") ‚Üí 10 + 20 + 15 = 45
 * 
 * 2. Compute sum of a numeric column grouped by a pivot column Return a map.
 * Example: sum("sales") grouped by "state" ‚Üí
 * CA ‚Üí 10 + 15 = 25
 * NY ‚Üí 20
 * 
 * Edge Cases
 * Column may not exist ‚Üí throw exception
 * Numeric columns stored as strings ‚Üí must parse
 * Extra columns may exist
 * pivotColumn may be:
 * categorical (state)
 * date (timestamp)
 * product
 * Empty table ‚Üí return empty map
 */
public class PivotTableAnalyzer {

    // Get column index by name
    private static int getColIndex(String[] header, String colName) {
        for (int i = 0; i < header.length; i++) {
            if (header[i].equals(colName)) return i;
        }
        throw new IllegalArgumentException("Column not found: " + colName);
    }

    // 1. Total sum of a numeric column
    public static double totalSum(String[][] table, String colName) {
        String[] header = table[0];
        int col = getColIndex(header, colName);

        double sum = 0;
        for (int i = 1; i < table.length; i++) {
            sum += Double.parseDouble(table[i][col]);
        }
        return sum;
    }

    // 2. Sum of a numeric column grouped by pivot column
    public static Map<String, Double> sumByPivot(String[][] table, String valueCol, String pivotCol) {
        String[] header = table[0];
        int valIdx = getColIndex(header, valueCol);
        int pivotIdx = getColIndex(header, pivotCol);

        Map<String, Double> result = new HashMap<>();

        for (int i = 1; i < table.length; i++) {
            String pivotValue = table[i][pivotIdx];
            double val = Double.parseDouble(table[i][valIdx]);

            result.put(pivotValue, result.getOrDefault(pivotValue, 0.0) + val);
        }

        return result;
    }

    // 3. Profit grouped by pivot column
    public static Map<String, Double> profitByPivot(String[][] table, String pivotCol) {
        String[] header = table[0];
        int salesIdx = getColIndex(header, "sales");
        int costIdx = getColIndex(header, "cost");
        int pivotIdx = getColIndex(header, pivotCol);

        Map<String, Double> profitMap = new HashMap<>();

        for (int i = 1; i < table.length; i++) {
            String pivotValue = table[i][pivotIdx];
            double sales = Double.parseDouble(table[i][salesIdx]);
            double cost = Double.parseDouble(table[i][costIdx]);

            profitMap.put(pivotValue,
                profitMap.getOrDefault(pivotValue, 0.0) + (sales - cost));
        }

        return profitMap;
    }

    // 4. Aggregate by timestamp and return the timestamp with the max sum 
    public static String maxByTimestamp(String[][] table, String valueCol) { 
        String[] header = table[0]; 
        int timestampIdx = getColIndex(header, "timestamp"); 
        int valueIdx = -1; 
        boolean isProfit = valueCol.equals("profit"); 
        int salesIdx = -1, costIdx = -1;
        if (isProfit) { 
            salesIdx = getColIndex(header, "sales"); 
            costIdx = getColIndex(header, "cost"); 
        } else { 
            valueIdx = getColIndex(header, valueCol); 
        } 
        Map<String, Double> agg = new HashMap<>(); 
        for (int i = 1; i < table.length; i++) { 
            String ts = table[i][timestampIdx]; 
            double val; 
            if (isProfit) { 
                double sales = Double.parseDouble(table[i][salesIdx]); 
                double cost = Double.parseDouble(table[i][costIdx]); 
                val = sales - cost; 
            } else { 
                val = Double.parseDouble(table[i][valueIdx]); 
            } 
            agg.put(ts, agg.getOrDefault(ts, 0.0) + val); 
        } 
        // Find timestamp with max aggregated value 
        String bestTs = null; 
        double bestVal = Double.NEGATIVE_INFINITY; 
        for (Map.Entry<String, Double> e : agg.entrySet()) { 
            if (e.getValue() > bestVal) { 
                bestVal = e.getValue(); 
                bestTs = e.getKey(); 
            } 
        } 
        // streaming manner
        return agg.entrySet().stream().max(Map.Entry.comparingByValue()).get().getKey();
        // return bestTs;
    }

    // Example usage
    public static void main(String[] args) {
        String[][] table = {
            {"prodId", "sales", "cost", "state", "timestamp"},
            {"A1", "10", "5", "CA", "2024-01-01"}, 
            {"A2", "20", "8", "NY", "2024-01-01"}, 
            {"A1", "15", "7", "CA", "2024-01-02"}, 
            {"A3", "30", "10", "TX", "2024-01-02"}
        };

        System.out.println("Total sales = " + totalSum(table, "sales"));
        System.out.println("Sales by state = " + sumByPivot(table, "sales", "state"));
        System.out.println("Profit by state = " + profitByPivot(table, "state"));
        System.out.println("Max sales timestamp = " + maxByTimestamp(table, "sales")); 
        System.out.println("Max profit timestamp = " + maxByTimestamp(table, "profit"));
    }
}

/**
 * If you wanted to take your Pivot Table Profit Analyzer from ‚Äúinterview code‚Äù
 * to a real production‚Äëgrade service, you‚Äôd need to evolve it from a simple
 * in‚Äëmemory calculator into a scalable, observable, reliable analytics
 * microservice. Here‚Äôs how a staff‚Äëlevel engineer would frame the
 * transformation.
 * 
 * üåê 1. Clarify the Real Production Use Case Before building anything, define
 * what the service must support:
 * 
 * Large datasets (millions of rows, not a tiny 2D array)
 * 
 * Multiple aggregations (sum, profit, max, min, avg)
 * 
 * Multiple pivot dimensions (state, date, product, etc.)
 * 
 * Filtering (WHERE clauses)
 * 
 * High concurrency (many clients calling at once)
 * 
 * Low latency (sub‚Äë100ms responses)
 * 
 * Strong correctness guarantees
 * 
 * This shifts the service from ‚Äútoy pivot table‚Äù to ‚Äúreal analytics engine.‚Äù
 * 
 * ‚öôÔ∏è 2. Architecture Upgrade Current (interview) Everything computed in memory
 * 
 * Single request ‚Üí single calculation
 * 
 * No persistence
 * 
 * No caching
 * 
 * No parallelism
 * 
 * Production A real service would look more like this:
 * 
 * Code Client ‚Üí API Gateway ‚Üí Pivot Analytics Service ‚Üí Cache ‚Üí Data Warehouse
 * ‚ÜòÔ∏é Observability Stack Components: Pivot Analytics Service Stateless
 * microservice
 * 
 * Performs aggregations, grouping, filtering
 * 
 * Supports multiple aggregation types
 * 
 * Caching Layer (Redis) Cache results of common queries
 * 
 * Cache parsed column indices
 * 
 * Cache pre‚Äëaggregated data (e.g., daily sales)
 * 
 * Data Warehouse BigQuery / Snowflake / Redshift / Spark
 * 
 * The service should push down heavy queries to the warehouse
 * 
 * Observability Metrics (latency, cache hit ratio, error rate)
 * 
 * Tracing (OpenTelemetry)
 * 
 * Structured logs
 * 
 * üöÄ 3. Performance Improvements 1. Column Index Pre‚Äëcomputation Instead of
 * scanning headers every request, pre‚Äëcompute:
 * 
 * Code Map<String, Integer> columnIndexCache 2. Vectorized Computation Process
 * columns in batches, not row‚Äëby‚Äërow.
 * 
 * 3. Parallel Aggregation Use parallel streams or thread pools for large
 * datasets.
 * 
 * 4. Pre‚Äëaggregated Materialized Views For example:
 * 
 * Daily sales per state
 * 
 * Daily profit per product
 * 
 * This reduces query time from seconds ‚Üí milliseconds.
 * 
 * 5. Query Caching Cache key:
 * 
 * Code hash(query parameters + date range) Value:
 * 
 * Code aggregated result 
 * 
 * üß± 4. Reliability Improvements 
 * 
 * 1. Input Validation
 * Reject invalid:
 * 
 * Column names
 * 
 * Timestamps
 * 
 * Non‚Äënumeric values in numeric columns
 * 
 * Empty tables
 * 
 * 2. Error Handling Return structured errors:
 * 
 * json { "error": "INVALID_COLUMN", "message": "Column 'profitz' does not
 * exist" } 3. Timeouts Prevent long‚Äërunning queries:
 * 
 * 100ms timeout for in‚Äëmemory
 * 
 * 1‚Äì3s timeout for warehouse queries
 * 
 * 4. Rate Limiting Protect the service from abuse.
 * 
 * üì° 5. Observability Metrics 
 * 
 * pivot_request_count
 * 
 * pivot_latency_ms
 * 
 * pivot_cache_hit_ratio
 * 
 * pivot_invalid_query_count
 * 
 * pivot_warehouse_query_time
 * 
 * Tracing Each aggregation step is a span
 * 
 * Warehouse calls are traced separately
 * 
 * Logging Structured logs with:
 * 
 * requestId
 * 
 * pivot column
 * 
 * aggregation type
 * 
 * row count
 * 
 * errors
 * 
 * üìà 6. Scalability 
 * 
 * 1. Stateless Microservice All state externalized ‚Üí easy
 * horizontal scaling.
 * 
 * 2. Load Balancing Round‚Äërobin or least‚Äëconnections.
 * 
 * 3. Sharding If storing pre‚Äëaggregated data:
 * 
 * Shard by date
 * 
 * Shard by product category
 * 
 * 4. Async Batch Jobs For large datasets:
 * 
 * Client submits job
 * 
 * Service processes asynchronously
 * 
 * Client polls for result
 * 
 * üîê 7. Security 
 * 
 * 1. Input Sanitization Prevent injection attacks:
 * 
 * Only allow alphanumeric column names
 * 
 * Validate timestamps
 * 
 * Reject overly large payloads
 * 
 * 2. Authentication OAuth2 / mTLS for internal services
 * 
 * 3. Authorization Some users may only access certain columns or states
 * 
 * üß™ 8. Testing Strategy 
 * 
 * Unit Tests Column lookup
 * 
 * Aggregation logic
 * 
 * Profit calculation
 * 
 * Timestamp parsing
 * 
 * Integration Tests Multiple pivot columns
 * 
 * Large datasets
 * 
 * Missing columns
 * 
 * Load Tests 10M rows
 * 
 * 1000 requests/sec
 * 
 * Chaos Tests Random missing columns
 * 
 * Random malformed rows
 * 
 * Random nulls
 * 
 * üéØ 9. Summary: What Makes It Production‚ÄëReady? Area Improvements Performance
 * Caching, parallelism, pre‚Äëaggregation Reliability Validation, error handling,
 * timeouts Observability Metrics, logs, tracing Scalability Stateless service,
 * load balancing Security Sanitization, auth, rate limiting Product Features
 * Flexible API, multiple aggregations Testing Unit, integration, load, chaos
 */